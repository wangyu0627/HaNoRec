import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def merge_lora(base_model_name, sft_lora_path, save_path):
    print(f"ðŸ”µ Loading base model: {base_model_name}")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    print(f"ðŸŸ¢ Loading SFT LoRA from: {sft_lora_path}")
    model = PeftModel.from_pretrained(model, sft_lora_path, is_trainable=False)

    print("ðŸ§© Merging LoRA weights into base model...")
    model = model.merge_and_unload()

    print(f"ðŸ’¾ Saving merged model to: {save_path}")
    model.save_pretrained(save_path, safe_serialization=False)

    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    tokenizer.save_pretrained(save_path)

    print("âœ… Merge and save completed.")

if __name__ == "__main__":
    # ðŸ‘‰ 
    base_model_name = "/data/my_name/MLLMRec/Qwen-2.5-3B-Instruct"  # or absolute path to HF model
    sft_lora_path = "/data/my_name/MLLMRec/checkpoints/netflix/sft_3_lora_qwen"
    save_path = "/data/my_name/MLLMRec/Qwen-2.5-3B-Instruct-SFT-Netflix_3"

    merge_lora(base_model_name, sft_lora_path, save_path)
